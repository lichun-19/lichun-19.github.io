---
---

@string{aps = {American Physical Society,}}

@article{pai2025billy,
  title={BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation},
  author={Pai, Tsung-Min and Wang, Jui-I and Lu, Li-Chun and Sun, Shao-Hua and Lee, Hung-Yi and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2510.10157},
  preview={billy_preview.png},
  abstract={Multi-LLM systems enhance the creativity of large language models by simulating human collective intelligence but suffer from significant drawbacks, such as high computational costs and inference latency. To address these limitations, we propose BILLY (BlendIng persona vectors for Large Language model creativitY), a training-free framework that captures the benefits of multi-LLM collaboration, i.e. inducing diverse perspectives and specialized expertise, within a single model. BILLY operates by extracting and blending multiple distinct persona vectors directly in the model's activation space. We steer the model's generation process with this merged vector while inference, enabling multi-perspective output without explicit multi-LLM communication. Our experiments across creativity-oriented benchmarks demonstrate that BILLY surpasses single model prompting and traditional multi-LLM approaches, while substantially reducing inference time and computational costs. Our analyses further reveal that distinct persona vectors can be blended to achieve both effective control over complementary aspects of generation and greater interpretability.},
  bibtex_show={true},

  year={2025}
}


@article{lu2025rethinking,
      bibtex_show={true},
      title={Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations},
      author={Lu, Li-Chun and Liu, Miri and Lu, Pin-Chun and Tian, Yufei and Sun, Shao-Hua and Peng, Nanyun},
      abstract={We systematically examine, analyze, and compare representative creativity measures--creativity index, perplexity, syntactic templates, and LLM-as-a-Judge--across diverse creative domains, including creative writing, unconventional problem-solving, and research ideation. Our analyses reveal that these metrics exhibit limited consistency, capturing different dimensions of creativity. We highlight key limitations, including the creativity index's focus on lexical diversity, perplexity's sensitivity to model confidence, and syntactic templates' inability to capture conceptual creativity. Additionally, LLM-as-a-Judge shows instability and bias. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.},
      journal={arXiv preprint arXiv:2508.05470},
      year={2025},
      selected={true},
      preview={rethinking_evaluation_preview.png}
}

@inproceedings{lullm,
      bibtex_show={true},
      title={LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play},
      author={Lu*, Li-Chun and Chen*, Shou-Jen and Pai, Tsung-Min and Yu, Chan-Hung and Lee, Hung-yi and Sun, Shao-Hua},
      booktitle={First Conference on Language Modeling},
      abstract={Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study. The results show that our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics.},
      year={2024},
      annotation={* Equal contribution},
      abbr={COLM '24},
      selected={true},
      code={https://github.com/lawraa/LLM-Discussion},
      url={https://arxiv.org/abs/2405.06373},
      preview={llmDiscussion_preview.png}
}


@inproceedings{10.1145/3613904.3642064,
      bibtex_show={true},
      author = {Lu, Pin-Chun and Wang, Che-Wei and Hsu, Yu Lun and Lopez, Alvaro and Tsai, Ching-Yi and Chang, Chiao-Ju and Tan, Wei Tian Mireille and Lu, Li-Chun and Chen, Mike Y.},
      title = {VeeR: Exploring the Feasibility of Deliberately Designing VR Motion that Diverges from Mundane, Everyday Physical Motion to Create More Entertaining VR Experiences},
      year = {2024},
      isbn = {9798400703300},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3613904.3642064},
      doi = {10.1145/3613904.3642064},
      abstract = {This paper explores the feasibility of deliberately designing VR motion that diverges from users’ physical movements to turn mundane, everyday transportation motion (e.g., metros, trains, and cars) into more entertaining VR motion experiences, in contrast to prior car-based VR approaches that synchronize VR motion to physical car movement exactly. To gain insight into users’ preferences for veering rate and veering direction for turning (left/right) and pitching (up/down) during the three phases of acceleration (accelerating, cruising, and decelerating), we conducted a formative, perceptual study (n=24) followed by a VR experience evaluation (n=18), all conducted on metro trains moving in a mundane, straight-line motion. Results showed that participants preferred relatively high veering rates, and preferred pitching upward during acceleration and downward during deceleration. Furthermore, while veering decreased comfort as expected, it significantly enhanced immersion (p<.01) and entertainment (p<.001) and the overall experience, with comfort being considered, was preferred by 89\% of participants.},
      booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
      articleno = {424},
      numpages = {13},
      abbr={CHI '24},
      preview = {veer_preview.png}
}

@inproceedings{10.1145/3613905.3648102,
      bibtex_show={true},
      author = {Hsu, Yu Lun and Lu, Chien-Ting and Lu, Li-Chun and Tam, Chih-Heng and Sun, Yu-Chieh and Wang, Ting-Kang},
      title = {AnimalSense: Understanding Beyond-human Sensory Capabilities of Animals via VR Games},
      year = {2024},
      isbn = {9798400703317},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3613905.3648102},
      doi = {10.1145/3613905.3648102},
      abstract = {Animals have sensory capabilities far beyond those of humans, including the ability to detect ultrasound, UV light, electric fields, magnetic fields, and to have panoramic vision. AnimalSense is a VR game crafted to immerse players in the extraordinary sensory worlds of animals. The game’s design leverages the concepts of sensory substitution, sensory remapping, and active learning, allowing users to explore and utilize these beyond-human sensory capabilities to overcome game challenges, thereby enhancing players’ understanding of animals’ unique senses. This paper presents three such game section designs: 1) echolocation in bats, 2) electroreception in eels, and 3) panoramic vision in spiders, showcasing how these are integrated into the gameplay.},
      booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
      articleno = {632},
      numpages = {6},
      keywords = {Animal Comprehension, Animals, Perceptual ranges, Sensory capabilities, Virtual Reality},
      abbr = {CHI '24 SGC},
      preview = {animal_sense_preview.png}
}


@inproceedings{huangdynamic,
      bibtex_show={true},
      title={Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks}, 
      author={Chien-yu Huang and Wei-Chih Chen and Shu-wen Yang and Andy T. Liu and Chen-An Li and Yu-Xiang Lin and Wei-Cheng Tseng and Anuj Diwan and Yi-Jen Shih and Jiatong Shi and William Chen and Xuanjun Chen and Chi-Yuan Hsiao and Puyuan Peng and Shih-Heng Wang and Chun-Yi Kuan and Ke-Han Lu and Kai-Wei Chang and Chih-Kai Yang and Fabian Ritter-Gutierrez and Ming To Chuang and Kuan-Po Huang and Siddhant Arora and You-Kuan Lin and Eunjung Yeo and Kalvin Chang and Chung-Ming Chien and Kwanghee Choi and Cheng-Hsiu Hsieh and Yi-Cheng Lin and Chee-En Yu and I-Hsiang Chiu and Heitor R. Guimarães and Jionghao Han and Tzu-Quan Lin and Tzu-Yuan Lin and Homu Chang and Ting-Wu Chang and Chun Wei Chen and Shou-Jen Chen and Yu-Hua Chen and Hsi-Chun Cheng and Kunal Dhawan and Jia-Lin Fang and Shi-Xin Fang and Kuan-Yu Fang Chiang and Chi An Fu and Hsien-Fu Hsiao and Ching Yu Hsu and Shao-Syuan Huang and Lee Chen Wei and Hsi-Che Lin and Hsuan-Hao Lin and Hsuan-Ting Lin and Jian-Ren Lin and Ting-Chun Liu and Li-Chun Lu and Tsung-Min Pai and Ankita Pasad and Shih-Yun Shan Kuan and Suwon Shon and Yuxun Tang and Yun-Shao Tsai and Jui-Chiang Wei and Tzu-Chieh Wei and Chengxi Wu and Dien-Ruei Wu and Chao-Han Huck Yang and Chieh-Chi Yang and Jia Qi Yip and Shao-Xiang Yuan and Vahid Noroozi and Zhehuai Chen and Haibin Wu and Karen Livescu and David Harwath and Shinji Watanabe and Hung-yi Lee},
      booktitle={The Thirteenth International Conference on Learning Representations},
      year={2025},
      abstract = {Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results indicate that none of the models performed well universally. SALMONN-13B excelled in English ASR, while WavLLM demonstrated high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We will soon open-source all task data and the evaluation pipeline.},
      abbr = {ICLR '25},
      url={https://arxiv.org/abs/2411.05361}, 
      preview = {dynamic_superb2_preview.png}
}

